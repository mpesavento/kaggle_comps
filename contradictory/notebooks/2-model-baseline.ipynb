{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04f0395-7e7d-4879-93da-fa0be5e7b1ea",
   "metadata": {},
   "source": [
    "# Contradictory sentences - baseline model\n",
    "Create a baseline model for contradiction classification\n",
    "\n",
    "Because this dataset is multi-lingual, we need to choose the best in class language model that is readily trainable (on kaggle TPUs?). One possibility is the [`XLM-RoBERTa`](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-base-multinerd) model, but this model has fallen out of favor due to major tokenization limitations. The preferred model for multilanguage NER is this SpanMarker model using xlm-roberta-base as the underlying encoder, trained on the multinerd dataset: [`span-marker-xlm-roberta-base-multinerd`](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-base-multinerd). The problem is that I wont want to be doing NER, I want to be doing sentence comparison.\n",
    "\n",
    "A reasonable starting point is just the base [`XLM-RoBERTa`](https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/xlm-roberta) model.\n",
    "\n",
    "This kaggle challenge was started as a reason to learn to use TPUs. You can use TPUs in PyTorch with the [`torch_xla`](https://pytorch.org/xla/release/2.0/index.html) package. See how to use it in this example kaggle code [here](https://www.kaggle.com/code/tanlikesmath/the-ultimate-pytorch-tpu-tutorial-jigsaw-xlm-r).  \n",
    "For now, we will stick with CPU/GPU. Double check Apple silicon MPS devices [[ref]](https://developer.apple.com/metal/pytorch/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717ed62d-39f5-44bd-b369-3e38749c5254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kaggle\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from transformers import (\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig)\n",
    "from datasets import DatasetDict\n",
    "import evaluate\n",
    "\n",
    "from utils import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants\n",
    "DATA_PATH = \"data\"\n",
    "WANDB_PROJECT = \"contradictory\"\n",
    "RAW_DATA_AT = \"contra_raw\"\n",
    "PROCESSED_DATA_AT = \"contra_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6066b84d-9a26-402a-b14f-6653ed60b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # random.seed(seed)\n",
    "    \n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9207d477-5454-4b86-ac40-51a716b39da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Found GPU: \", torch.cuda.device_count())\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"Found MPS, may not work on some torch ops!\" )\n",
    "    device = \"mps\"\n",
    "\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830b58fa-5268-4f42-b934-dd56a5f972dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global parameters\n",
    "MODEL_NAME = \"xlm-roberta-base\"  # \"xlm-roberta-large\"\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 16 # hyperparameter, can iterate on this later\n",
    "\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553e6fcc-ed15-45c6-af67-7402f13d387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(DATA_PATH, f\"contradiction-training-{str(int(time.time()))}\")\n",
    "\n",
    "train_config = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=f\"{MODEL_NAME}-baseline\",  # name of the W&B run (optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30e30b9-9e3e-4cca-9117-8ef37f67fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmpesavento\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/kaggle_comps/contradictory/notebooks/wandb/run-20230817_014006-9khspxwx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mpesavento/contradictory/runs/9khspxwx' target=\"_blank\">hopeful-dragon-31</a></strong> to <a href='https://wandb.ai/mpesavento/contradictory' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mpesavento/contradictory' target=\"_blank\">https://wandb.ai/mpesavento/contradictory</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mpesavento/contradictory/runs/9khspxwx' target=\"_blank\">https://wandb.ai/mpesavento/contradictory/runs/9khspxwx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init wandb\n",
    "run = wandb.init(project=WANDB_PROJECT, entity=None, job_type=\"training\", config=train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3598a7ca-4a06-4648-a5b9-c147ed396d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "processed_data_at = run.use_artifact(f'{PROCESSED_DATA_AT}:latest')\n",
    "processed_dataset_dir = Path(processed_data_at.download())\n",
    "df = pd.read_csv(processed_dataset_dir / 'data_split.csv')\n",
    "\n",
    "# drop test for now, split in valid & train\n",
    "df = df[df.Stage != 'test'].reset_index(drop=True)\n",
    "df['is_valid'] = df.Stage == 'valid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec85a4a-cd31-42b8-af45-e79c3bdf5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d27bc8d-a35a-4723-a873-30e611eb4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df[df[\"is_valid\"]!=True])\n",
    "valid_dataset = Dataset.from_pandas(df[df[\"is_valid\"]])\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"validation\": valid_dataset})\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63db7120-4629-4d7e-8c53-ced1e5fa3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_batch(examples):\n",
    "    tokenized_examples = tokenizer(examples[\"premise\"], examples[\"hypothesis\"], \n",
    "                                   truncation=True, padding=True, return_tensors=\"pt\",)\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ab4a8c-3f43-45da-bb8c-eb5afe84cdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b4881edc624e639d2d827e48844524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9696 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae3a204b92a4f9a9efe60cd0cf5eb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83c7f52-f403-4c85-a291-113cff95eae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> They look just as good as new.\" They cut them carefully and ripped away the oilskin.</s></s> The oilskin would be good for several months of use.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70393836-6742-42ac-ad45-a6999aed4cb3",
   "metadata": {},
   "source": [
    "# data collator\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollator:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fe9aa70-3f3c-4489-8f41-18077fe3dbc6",
   "metadata": {},
   "source": [
    "# original was from CustomRoberta here: https://www.kaggle.com/code/tanlikesmath/the-ultimate-pytorch-tpu-tutorial-jigsaw-xlm-r?scriptVersionId=37280514&cellId=27\n",
    "# this is an attempt to manually set up a sequence classifier. It looks like i'm not calculating the loss correctly, likely from \n",
    "# not interpreting the source code correctly\n",
    "# Using the XLMRobertaForSequenceClassification is the correct way to do this!\n",
    "# check source code for important differences: https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L1162\n",
    "\n",
    "class XLMRobertaContradict(nn.Module):\n",
    "    def __init__(self, dropout=0.0):\n",
    "        super(XLMRobertaContradict, self).__init__()\n",
    "        self.num_labels = 3\n",
    "        self.dropout_pct = dropout\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(MODEL_NAME, \n",
    "                                                       output_hidden_states=False, num_labels=self.num_labels)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_pct)\n",
    "        self.feats = self.roberta.pooler.dense.out_features\n",
    "        self.relu =  torch.nn.ReLU(inplace=True)\n",
    "        self.linear = nn.Linear(self.feats, self.num_labels)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                # labels=None,\n",
    "               ):\n",
    "        x = self.roberta(input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       position_ids=position_ids,\n",
    "                       head_mask=head_mask,\n",
    "                       inputs_embeds=inputs_embeds)\n",
    "        # these have an unknown effect, idk what the hid_mix is doing. \n",
    "        # maybe blending the last hidden states to extend the output embedding?\n",
    "        # hidden_states = x[2]\n",
    "        # hmix = []\n",
    "        # for i in range(1, self.hid_mix + 1):\n",
    "        #     hmix.append(hidden_states[-i][:, 0].reshape((-1, 1, self.feats)))\n",
    "        # hmix_tensor = torch.cat(hmix, 1)\n",
    "        # mean_tensor = torch.mean(hmix_tensor, 1)\n",
    "        # pool_tensor = self.dropout(mean_tensor)\n",
    "        # return self.classifier(pool_tensor)\n",
    "        \n",
    "        # \n",
    "        # x = x.pooler_output  # get just the [CLS] token\n",
    "        x = x.last_hidden_state[:,0,:]\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        return x\n",
    "\n",
    "# model = XLMRobertaContradict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6699eeb8-4a87-4e48-b524-2fe7b161ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_fn = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    ent_ix = np.where(labels==label2id[\"entailment\"])[0]\n",
    "    neut_ix = np.where(labels==label2id[\"neutral\"])[0]\n",
    "    contra_ix = np.where(labels==label2id[\"contradiction\"])[0]\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_fn.compute(\n",
    "            predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"acc_entailment\": accuracy_fn.compute(\n",
    "            predictions=predictions[ent_ix], references=labels[ent_ix])[\"accuracy\"],\n",
    "        \"acc_neutral\": accuracy_fn.compute(\n",
    "            predictions=predictions[neut_ix], references=labels[neut_ix])[\"accuracy\"],\n",
    "        \"acc_contradiction\": accuracy_fn.compute(\n",
    "            predictions=predictions[contra_ix], references=labels[contra_ix])[\"accuracy\"],\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2174d3-4fa0-429a-83ee-568f74c24479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "num_labels = len(np.unique(tokenized_datasets['train'][\"label\"]))\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598d262d-c95f-4c6c-ad27-b87d94cdeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_config,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f18eb81-f785-4ec5-90fb-9e6eeac0688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6060' max='6060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6060/6060 15:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Acc Entailment</th>\n",
       "      <th>Acc Neutral</th>\n",
       "      <th>Acc Contradiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.036300</td>\n",
       "      <td>0.905834</td>\n",
       "      <td>0.580858</td>\n",
       "      <td>0.581340</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.362069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>0.758498</td>\n",
       "      <td>0.681518</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.610825</td>\n",
       "      <td>0.647783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>0.770384</td>\n",
       "      <td>0.699670</td>\n",
       "      <td>0.705742</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.692118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.183000</td>\n",
       "      <td>0.869413</td>\n",
       "      <td>0.702970</td>\n",
       "      <td>0.746411</td>\n",
       "      <td>0.675258</td>\n",
       "      <td>0.684729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.950777</td>\n",
       "      <td>0.707096</td>\n",
       "      <td>0.746411</td>\n",
       "      <td>0.662371</td>\n",
       "      <td>0.709360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train it!\n",
    "model_trained = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cc681d9-8662-41f2-beb8-483cf6ea3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions_table(dataset, id2label):\n",
    "    \"\"\"Creates a wandb table with predictions and targets side by side\"\"\"\n",
    "    predictions = trainer.predict(dataset, metric_key_prefix=\"validate\")\n",
    "    X_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_labels = predictions.label_ids\n",
    "    if not np.array_equal(y_labels, [dataset[i][\"label\"] for i in range(len(dataset))]):\n",
    "        raise ValueError(\"prediction labels do not match dataset labels\")\n",
    "    \n",
    "    col_names = [\"id\", \"premise\", \"hypothesis\", \"lang_abv\", \"label\", \"predict\"]\n",
    "\n",
    "    data_out = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        data_out.append({\n",
    "            col:sample[col] for col in col_names[:-1]})\n",
    "        data_out[-1][col_names[-1]] = X_pred[i]\n",
    "\n",
    "    data_df = pd.DataFrame.from_records(data_out)\n",
    "    table = wandb.Table(data=data_df)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8688202-3f07-4031-9119-47fcd7da3bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1212it [00:00, 4139.07it/s]\n"
     ]
    }
   ],
   "source": [
    "table = create_predictions_table(tokenized_datasets['validation'], id2label)\n",
    "wandb.log({\"pred_table\":table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11e74f53-1047-4713-86ae-664c7a6534b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = trainer.evaluate()\n",
    "for k,v in scores.items():\n",
    "    wandb.summary[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b2106d3-592a-4ee8-866d-4376a3db612b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f1a696dbf0453e855faff660e5e506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.817 MB of 0.817 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/acc_contradiction</td><td>▁▇████</td></tr><tr><td>eval/acc_entailment</td><td>▁█▅▇▇▇</td></tr><tr><td>eval/acc_neutral</td><td>█▁▄▃▃▃</td></tr><tr><td>eval/accuracy</td><td>▁▇████</td></tr><tr><td>eval/loss</td><td>▆▁▁▅██</td></tr><tr><td>eval/runtime</td><td>▄▁█▄▆▁</td></tr><tr><td>eval/samples_per_second</td><td>▅█▁▅▃█</td></tr><tr><td>eval/steps_per_second</td><td>▅█▁▅▃█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇▇▇▇▆▄▅▅▇█▄▄▅▄▃▇▆▅▅▄▄▄▆▃▃▅▃▂▄▁▂▃▃▅▁▁▂▁▆▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5.0</td></tr><tr><td>eval/acc_contradiction</td><td>0.70936</td></tr><tr><td>eval/acc_entailment</td><td>0.74641</td></tr><tr><td>eval/acc_neutral</td><td>0.66237</td></tr><tr><td>eval/accuracy</td><td>0.7071</td></tr><tr><td>eval/loss</td><td>0.95078</td></tr><tr><td>eval/runtime</td><td>3.6988</td></tr><tr><td>eval/samples_per_second</td><td>327.678</td></tr><tr><td>eval/steps_per_second</td><td>41.095</td></tr><tr><td>eval_acc_contradiction</td><td>0.70936</td></tr><tr><td>eval_acc_entailment</td><td>0.74641</td></tr><tr><td>eval_acc_neutral</td><td>0.66237</td></tr><tr><td>eval_accuracy</td><td>0.7071</td></tr><tr><td>eval_loss</td><td>0.95078</td></tr><tr><td>eval_runtime</td><td>3.6988</td></tr><tr><td>eval_samples_per_second</td><td>327.678</td></tr><tr><td>eval_steps_per_second</td><td>41.095</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>6060</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1124</td></tr><tr><td>train/total_flos</td><td>6082166764524816.0</td></tr><tr><td>train/train_loss</td><td>0.69019</td></tr><tr><td>train/train_runtime</td><td>956.8719</td></tr><tr><td>train/train_samples_per_second</td><td>50.665</td></tr><tr><td>train/train_steps_per_second</td><td>6.333</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-dragon-31</strong> at: <a href='https://wandb.ai/mpesavento/contradictory/runs/9khspxwx' target=\"_blank\">https://wandb.ai/mpesavento/contradictory/runs/9khspxwx</a><br/> View job at <a href='https://wandb.ai/mpesavento/contradictory/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwOTc3MzA4/version_details/v5' target=\"_blank\">https://wandb.ai/mpesavento/contradictory/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwOTc3MzA4/version_details/v5</a><br/>Synced 6 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230817_014006-9khspxwx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39110a96-d556-4b52-9d69-ea54369851b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contradict",
   "language": "python",
   "name": "contradict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
